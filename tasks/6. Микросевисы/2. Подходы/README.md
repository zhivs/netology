# Домашнее задание к занятию «Микросервисы: подходы»

Вы работаете в крупной компании, которая строит систему на основе микросервисной архитектуры.
Вам как DevOps-специалисту необходимо выдвинуть предложение по организации инфраструктуры для разработки и эксплуатации.


## Задача 1: Обеспечить разработку

Предложите решение для обеспечения процесса разработки: хранение исходного кода, непрерывная интеграция и непрерывная поставка. 
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- облачная система;
- система контроля версий Git;
- репозиторий на каждый сервис;
- запуск сборки по событию из системы контроля версий;
- запуск сборки по кнопке с указанием параметров;
- возможность привязать настройки к каждой сборке;
- возможность создания шаблонов для различных конфигураций сборок;
- возможность безопасного хранения секретных данных (пароли, ключи доступа);
- несколько конфигураций для сборки из одного репозитория;
- кастомные шаги при сборке;
- собственные докер-образы для сборки проектов;
- возможность развернуть агентов сборки на собственных серверах;
- возможность параллельного запуска нескольких сборок;
- возможность параллельного запуска тестов.

Обоснуйте свой выбор.

# Решение для CI/CD процесса разработки

## Компоненты решения
1. **GitHub (система контроля версий)**:
   - Облачная платформа для хранения исходного кода с использованием Git.
   - Поддержка репозиториев для каждого сервиса.
   - Хранение исходного кода, управление ветками, pull requests и code reviews.

2. **GitHub Actions (CI/CD)**:
   - Облачный инструмент для автоматизации CI/CD процессов.
   - Запуск сборок по событиям (push, pull request) или вручную с параметрами.
   - Поддержка кастомных шагов, шаблонов и параллельных сборок/тестов.

3. **GitHub Packages**:
   - Хранилище для Docker-образов, интегрированное с GitHub.
   - Хранение собственных Docker-образов для сборки и развертывания.

4. **HashiCorp Vault**:
   - Облачное решение для безопасного хранения секретных данных (пароли, ключи доступа).
   - Интеграция с GitHub Actions для безопасного доступа к секретам.

## Принципы взаимодействия
1. **Хранение исходного кода**:
   - Каждый сервис имеет отдельный репозиторий в GitHub.
   - Разработчики используют Git для управления версиями (push, pull, branch, merge).
   - GitHub обеспечивает доступ через SSH/HTTPS с аутентификацией (OAuth, PAT).

2. **Непрерывная интеграция (CI)**:
   - **GitHub Actions** отслеживает события в репозитории (push, pull request) и запускает workflow.
   - Workflow определяются в файлах `.github/workflows/*.yml`, где задаются шаги сборки, тестирования и параметры.
   - Поддерживаются кастомные шаги (скрипты, команды, запуск Docker-контейнеров).
   - Возможность создания шаблонов workflow для различных конфигураций (например, dev, staging, prod).
   - Параллельные сборки и тесты выполняются через матричные стратегии (`matrix` в GitHub Actions).

3. **Непрерывная поставка (CD)**:
   - После успешной сборки и тестирования GitHub Actions деплоит артефакты (например, Docker-образы) в GitHub Packages.
   - Развертывание на целевые серверы (например, Kubernetes, AWS ECS) выполняется через кастомные шаги в workflow.
   - Поддержка нескольких конфигураций (dev, staging, prod) через переменные окружения и параметры workflow.

4. **Безопасность**:
   - Секреты (пароли, ключи API) хранятся в HashiCorp Vault.
   - GitHub Actions получает доступ к секретам через Vault API с временными токенами.
   - GitHub Secrets (встроенный механизм) используется для простых секретов, если Vault не требуется.

5. **Собственные агенты и Docker-образы**:
   - GitHub Actions поддерживает self-hosted runners, которые можно развернуть на собственных серверах для выполнения сборок.
   - Собственные Docker-образы создаются и хранятся в GitHub Packages, используются в workflow для кастомных окружений.

6. **Запуск сборок**:
   - **По событию**: Workflow запускается автоматически при push, pull request или других событиях Git.
   - **По кнопке**: Используется `workflow_dispatch` для ручного запуска с параметрами (например, выбор ветки или окружения).
   - Параметры и настройки сборки задаются в YAML-файлах или через UI GitHub.

## Пример конфигурации GitHub Actions
Ниже приведён пример workflow для CI/CD с поддержкой всех требований:

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment (dev, staging, prod)'
        required: true
      version:
        description: 'Version tag'
        default: 'latest'

jobs:
  build-and-test:
    runs-on: [self-hosted, linux] # Использование собственного агента
    strategy:
      matrix:
        config: [dev, staging, prod] # Параллельные конфигурации
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3Salvatore

      - name: Build Docker Image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ghcr.io/${{ github.repository }}:${{ github.event.inputs.version || 'latest' }}

      - name: Run Tests
        run: docker run --rm ghcr.io/${{ github.repository }}:${{ matrix.config }}-test
        env:
          ENV: ${{ matrix.config }}

      - name: Login to GitHub Packages
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Push to GitHub Packages
        uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: ghcr.io/${{ github.repository }}:${{ matrix.config }}-${{ github.event.inputs.version || 'latest' }}

      - name: Deploy
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          docker run --rm -e VAULT_ADDR=${{ secrets.VAULT_ADDR }} \
          -e VAULT_TOKEN=${{ secrets.VAULT_TOKEN }} \
          my-deploy-image deploy ${{ matrix.config }}
```

## Интеграция с HashiCorp Vault
- Vault разворачивается в облаке (например, AWS, GCP) или на собственных серверах.
- GitHub Actions использует Vault GitHub Actions (`hashicorp/vault-action`) для получения секретов:
  ```yaml
  - name: Get secrets from Vault
    uses: hashicorp/vault-action@v2
    with:
      url: ${{ secrets.VAULT_ADDR }}
      token: ${{ secrets.VAULT_TOKEN }}
      secrets: secret/data/my-app/${{ matrix.config }} | MY_SECRET
  ```
- Секреты передаются в workflow как переменные окружения.

## Преимущества решения
- **Облачная система**: GitHub и GitHub Actions полностью облачные, Vault доступен в облаке.
- **Git и репозитории**: GitHub использует Git, поддерживает отдельные репозитории для каждого сервиса.
- **События и ручной запуск**: GitHub Actions поддерживает запуск по событиям и `workflow_dispatch` для ручного запуска с параметрами.
- **Настройки и шаблоны**: YAML-файлы позволяют задавать конфигурации и шаблоны для сборок.
- **Безопасность**: Vault обеспечивает безопасное хранение секретов, GitHub Secrets — дополнительная защита.
- **Множественные конфигурации**: Матричные стратегии и переменные окружения поддерживают разные окружения.
- **Кастомные шаги**: Поддержка скриптов, Docker и любых команд.
- **Docker-образы**: GitHub Packages хранит кастомные образы.
- **Self-hosted runners**: Возможность развертывания агентов на собственных серверах.
- **Параллельные сборки/тесты**: Матричные стратегии и независимые jobs обеспечивают параллелизм.

## Задача 2: Логи

Предложите решение для обеспечения сбора и анализа логов сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор логов в центральное хранилище со всех хостов, обслуживающих систему;
- минимальные требования к приложениям, сбор логов из stdout;
- гарантированная доставка логов до центрального хранилища;
- обеспечение поиска и фильтрации по записям логов;
- обеспечение пользовательского интерфейса с возможностью предоставления доступа разработчикам для поиска по записям логов;
- возможность дать ссылку на сохранённый поиск по записям логов.

Обоснуйте свой выбор.

# Решение для сбора и анализа логов в микросервисной архитектуре

Для реализации эффективного решения по сбору и анализу логов в микросервисной архитектуре предлагаю использовать **стек EFK (Elasticsearch, Fluentd, Kibana)** с дополнительным использованием **Kafka** для обеспечения надёжности доставки.

---

## Компоненты и их роль

### 1. **Fluentd**
- **Роль**: Агент сбора логов на уровне каждого хоста.
- **Функции**:
  - Сбор логов из `stdout` контейнеров или файлов.
  - Буферизация и отправка логов далее (в Kafka или напрямую в Elasticsearch).
  - Поддержка тегирования, фильтрации, преобразования форматов (например, JSON).
- **Преимущества**:
  - Лёгкий и гибкий.
  - Минимальное влияние на приложения.
  - Поддерживает буферизацию на диске, что помогает в случае временных сбоев.

> Можно использовать **Fluent Bit** как более легковесную замену Fluentd на уровне агентов.

---

### 2. **Kafka (Опционально, но рекомендуется)**
- **Роль**: Промежуточная очередь сообщений между агентами и центральным процессором логов.
- **Функции**:
  - Обеспечивает гарантированную доставку логов даже при краткосрочных простоях backend-систем.
  - Выступает как буфер и шина событий.
- **Преимущества**:
  - Высокая отказоустойчивость.
  - Возможность масштабирования и обработки больших объёмов данных.
  - Поддерживает репликацию и долгосрочное хранение партиций.

---

### 3. **Elasticsearch**
- **Роль**: Хранилище логов с поддержкой полнотекстового поиска и агрегаций.
- **Функции**:
  - Индексация логов.
  - Поддержка сложных запросов, фильтров, агрегаций.
- **Преимущества**:
  - Быстрый поиск.
  - Масштабируемость.
  - Поддержка распределённой архитектуры.

---

### 4. **Kibana**
- **Роль**: Веб-интерфейс для просмотра, поиска и анализа логов.
- **Функции**:
  - Поиск по логам с фильтрами.
  - Сохранение поисковых запросов и создание визуализаций.
  - Генерация ссылок на сохранённые поиски (Shareable URLs).
  - Назначение ролей и доступов разработчикам через RBAC.
- **Преимущества**:
  - Удобный UI.
  - Интеграция с Elasticsearch.
  - Поддержка сохранения и совместного использования результатов поиска.

---

## Как это соответствует требованиям

| Требование | Реализация |
|-----------|------------|
| Сбор логов в центральное хранилище со всех хостов | Fluentd устанавливается на все хосты и пересылает логи в общее хранилище (через Kafka или напрямую в ES) |
| Минимальные требования к приложениям, сбор из stdout | Fluentd собирает логи из stdout контейнеров без изменения кода |
| Гарантированная доставка логов | Использование Kafka или буферизации на диске в Fluentd обеспечивает надёжную доставку |
| Поиск и фильтрация по логам | Elasticsearch + Kibana предоставляют мощный интерфейс поиска и фильтрации |
| Пользовательский интерфейс для разработчиков | Kibana предоставляет удобный UI с возможностью настройки прав доступа |
| Возможность дать ссылку на сохранённый поиск | Kibana позволяет сохранять поисковые запросы и делиться ими через URL |

---

## Безопасность и доступ

- **RBAC в Kibana**: Разграничение прав между разработчиками, тестировщиками, DevOps.
- **Шифрование TLS**: Между компонентами (Fluentd → Kafka → Elasticsearch).
- **Аутентификация**: LDAP/AD или SSO через Keycloak, если используется Elastic Stack с Security (ранее X-Pack).

---

## Масштабируемость

- Все компоненты легко масштабируются горизонтально:
  - Добавление новых нод в Elasticsearch.
  - Расширение Kafka за счёт добавления брокеров.
  - Запуск Fluentd как DaemonSet в Kubernetes.

---

## Альтернативы

- **Loki + Promtail + Grafana**: Легковесное решение от CNCF, особенно хороша в Kubernetes-окружении. Но не обеспечивает столь мощного поиска, как Kibana+Elasticsearch.
- **ELK вместо EFK**: Logstash вместо Fluentd — больше функционала, но выше нагрузка на систему.
- **SaaS-решения**: Datadog, New Relic, Honeycomb — удобны, но дороже и менее гибкие.

---

## Вывод

Предложенное **EFK + Kafka** решение является:

- **Гибким**, так как позволяет собирать логи из любого источника.
- **Надёжным**, благодаря буферизации и очередям.
- **Удобным для разработчиков**, благодаря Kibana и возможности делиться ссылками.
- **Масштабируемым**, подходит для проектов любого размера.



## Задача 3: Мониторинг

Предложите решение для обеспечения сбора и анализа состояния хостов и сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор метрик со всех хостов, обслуживающих систему;
- сбор метрик состояния ресурсов хостов: CPU, RAM, HDD, Network;
- сбор метрик потребляемых ресурсов для каждого сервиса: CPU, RAM, HDD, Network;
- сбор метрик, специфичных для каждого сервиса;
- пользовательский интерфейс с возможностью делать запросы и агрегировать информацию;
- пользовательский интерфейс с возможностью настраивать различные панели для отслеживания состояния системы.

Обоснуйте свой выбор.

# Решение для сбора и анализа состояния хостов и сервисов в микросервисной архитектуре

---

## Компоненты и их роль

### 1. **Prometheus**
- **Роль**: Система мониторинга и сбора метрик времени.
- **Функции**:
  - Периодический опрос метрик по HTTP (pull-модель).
  - Хранение временных рядов.
  - Поддержка мощного языка запросов PromQL для фильтрации, агрегации и анализа данных.
- **Преимущества**:
  - Высокая производительность.
  - Простота настройки и масштабирования.
  - Отлично интегрируется с Grafana и другими системами.

---

### 2. **Node Exporter**
- **Роль**: Агент сбора метрик о состоянии хоста.
- **Функции**:
  - Сбор информации о CPU, RAM, HDD, Network и других ресурсах.
  - Доступ к метрикам через HTTP-эндпоинт `/metrics`.
- **Установка**: Устанавливается на каждый хост как отдельный демон.

---

### 3. **Service-Specific Exporters**
- **Роль**: Экспортеры метрик, специфичных для конкретного сервиса.
- **Примеры**:
  - PostgreSQL Exporter
  - MySQL Exporter
  - Redis Exporter
  - Nginx Exporter
  - Custom Application Metrics (через библиотеки Prometheus Client Libraries)
- **Интеграция**: Сервис может предоставлять свои метрики через `/metrics` или быть обёрнут в отдельный экспортер.

---

### 4. **Grafana**
- **Роль**: Визуализация и анализ метрик.
- **Функции**:
  - Создание дашбордов (панелей) с графиками, таблицами и диаграммами.
  - Поддержка PromQL для составления сложных запросов.
  - Возможность сохранения и совместного использования дашбордов.
  - Настройка прав доступа через RBAC.
- **Преимущества**:
  - Гибкий и мощный UI.
  - Поддерживает множество источников данных, включая Prometheus, Loki, Elasticsearch и другие.
  - Подходит для командной работы и мониторинга в реальном времени.

---

## Как это соответствует требованиям

| Требование | Реализация |
|-----------|------------|
| Сбор метрик со всех хостов | Node Exporter устанавливается на все хосты, Prometheus скрапит метрики |
| Сбор метрик состояния ресурсов хостов (CPU, RAM, HDD, Network) | Node Exporter предоставляет стандартные метрики |
| Сбор метрик потребляемых ресурсов для каждого сервиса | Prometheus собирает метрики из контейнеров Docker/Kubernetes или через sidecar-экспортеры |
| Сбор метрик, специфичных для каждого сервиса | Использование service-specific exporters или внедрение клиентских библиотек Prometheus в приложение |
| Пользовательский интерфейс с возможностью делать запросы и агрегировать информацию | Grafana + PromQL позволяют выполнять произвольные запросы и агрегации |
| Пользовательский интерфейс с возможностью настраивать различные панели | Grafana позволяет создавать и настраивать дашборды под нужды разных ролей (DevOps, разработчики, SRE) |

---

## Безопасность и доступ

- **RBAC в Grafana**: Разграничение прав между пользователями.
- **Аутентификация и шифрование**:
  - TLS для Prometheus и Grafana.
  - Аутентификация через OAuth, LDAP/AD или API-ключи.
- **Защита эндпоинтов `/metrics`**: Ограниченный доступ, использование Basic Auth или OIDC.

---

## Масштабируемость

- **Prometheus**:
  - Для очень больших систем можно использовать **federation** или перейти на **VictoriaMetrics** как более масштабируемую замену.
- **Node Exporter**:
  - Лёгковесный процесс, легко масштабируется на тысячи нод.
- **Grafana**:
  - Поддерживает работу с несколькими источниками данных и большим количеством пользователей.

---

## Альтернативы

| Решение | Описание |
|--------|----------|
| **Telegraf + InfluxDB + Grafana** | Push-подход, хорош для IoT и случаев, когда не хочется открывать порты для скрапинга |
| **Zabbix** | Классическая система мониторинга с алертингом и UI, но менее гибкая для микросервисов |
| **OpenTelemetry + Prometheus + Grafana** | Современный подход с единым форматом метрик, логов и трейсов |
| **SaaS-решения** | Datadog, New Relic — удобны, но платные и менее гибкие |

---

## Вывод

Предложенное решение на основе **Prometheus + Node Exporter + Grafana** является:

- **Гибким**, так как позволяет собирать метрики любого типа.
- **Надёжным**, благодаря pull-архитектуре и стабильности Prometheus.
- **Удобным для анализа**, благодаря мощному PromQL и визуализации в Grafana.
- **Масштабируемым**, подходит как для маленьких, так и для крупных систем.

